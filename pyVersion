import tensorflow
import keras

import os
import pandas as pd
import numpy as np
import datetime as dt
import matplotlib.pyplot as plt
from pylab import rcParams
import seaborn as sns

from sklearn.metrics import confusion_matrix, precision_recall_curve,recall_score, classification_report, auc, roc_curve,precision_recall_fscore_support, f1_score

dat = pd.read_csv(" ")
dat.head()

dat['date'] = pd.to_datetime (dat['date']).dt.dayofyear
dat=dat.sort_values('')
dat.fillna(0, inplace=True)
y1 = dat['']
d = dat['']

dat.head()
#how frequent are failures?
sum(y1)/len(y1)

#normalize
import pandas as pd
from sklearn import preprocessing
dat.fillna(0, inplace=True)

df = dat
del df['']
cols = df.columns
df = pd.DataFrame(df)

min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(df)
df = pd.DataFrame(np_scaled, columns = cols)
df.head(n=15)

y1=df['failure']
del df['failure']
df['failure'] =y1

#lets look at variable distribution
#the features
X = df.iloc[:, :-1]
#whether or not a failure occured
y = df.iloc[:, -1]

failure = df.loc[y == 1]

no_failure = df.loc[y == 0]
# plots
plt.scatter(failure.iloc[:, 0], failure.iloc[:, 1], s=50, label='Failure')
plt.scatter(no_failure.iloc[:, 0], no_failure.iloc[:, 1], s=.01, label='No Failure')
plt.legend()
plt.show()

num= int(124494/2)
num

#let's split our test and train set
df=df.sort_values('')
df['']=d
df1= df.tail(n=num) #test
df = df.head(n=num) #train

#then group training data by failure=1 and the two proceeding rows
##this resampling should increase our sensitivity with the normal test data
n=3
df2= df[pd.concat([df.failure.shift(-i)==1 for i in range(n)], axis=1).any(axis=1)]
df2=df2.sort_values('')
df2

#dont need device label any more for the time series data
#df2 (train) and df2 (test) should have the same number of rows (11) but test should have many more columns
del df2['']
del df1['']

v=len(df2)/2
print(df2.shape, df1.shape)
df2

#get y values
from numpy import array

y1=df2['failure']
train_y= array(y1)
del df2['failure']
train_X = array(df2)

test_y = array(df1['failure'])
del df1['failure']
test_X = array(df1)
print(train_X.shape, test_X.shape)

#train_y = keras.utils.to_categorical(train_y)
#test_y = keras.utils.to_categorical(test_y)

#pad training data to value that is divisible by 32 (somewhere in the LSTM it is insistent that batch is divisble by 32)
f = [ [0] * 10 for _ in range(192)]
f = array(f)
result = np.zeros(f.shape)

result[:train_X.shape[0],:train_X.shape[1]] = train_X

result.shape

#pad training y values
train_y = array(y1).reshape((150,1))
#test_y = array(test_y).reshape((159,1))

f = [ [0] * 1 for _ in range(192)]
f = array(f)
result3 = np.zeros(f.shape)

result3[:train_y.shape[0],:] = train_y #train_y.shape[1]

result3.shape

#reshape data into arrays
train_X=result
train_y=result3

train_X = array(train_X).reshape((train_X.shape[0], 1, train_X.shape[1]))  #192,1,10
test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))

test_y = array(test_y).reshape((62247,1))

print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.models import Sequential

model = Sequential()
model.add(LSTM(128, input_shape=(None, train_X.shape[2]), batch_size=32)) #return_sequences=True, #, stateful=True
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())
#increase the class_weight of failure=1 to int value of 2 so that model doesn't overfit for nulls
class_weight = {0: 1.,
                1: 3.}  
#run model
history = model.fit(train_X, train_y, epochs=3000, batch_size=32, verbose=0, shuffle=False, class_weight=class_weight)

# plot history
plt.plot(history.history['loss'], label='test')
plt.legend()
plt.show()

plt.plot(history.history['acc'], label='test')
plt.legend()
plt.show()

print(history.history.get('loss')[-1], history.history.get('acc')[-1])

# calculate our prediction values
yhat = model.predict_classes(test_X)  #predict_classes
#since we are doing classification, output should be an integer value of all the positives
print(sum(yhat),len(yhat))

#pred_y = model.predict(test_X).ravel()
fpr, tpr, thresholds_keras = roc_curve(test_y, yhat)

from sklearn.metrics import auc
auc_keras = auc(fpr, tpr)
print("The area under the curve is ", auc_keras)

#precision and recall
precision, recall, threshold = precision_recall_curve(test_y, yhat)
plt.plot(threshold, precision[1:], label="Precision/ Specificity",linewidth=5)
plt.plot(threshold, recall[1:], label="Recall/ Sensitivty",linewidth=5)
plt.title('Precision and recall for different threshold values')
plt.xlabel('Threshold')
plt.ylabel('Precision/Recall')
plt.legend()
plt.show()

# plot ROC
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % auc_keras)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

#confusion matrix
rcParams['figure.figsize'] = 8, 6
LABELS = ["Normal","Break"]

conf_matrix = confusion_matrix(test_y, yhat)
plt.figure(figsize=(12, 12))
sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt="d");
plt.title("Confusion matrix")
plt.ylabel('True class')
plt.xlabel('Predicted class')
plt.show()

conf_matrix






